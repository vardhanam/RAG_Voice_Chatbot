{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23ed3b4a-1915-4b4f-9cf5-381885cd0912",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "AutoTokenizer,\n",
    "AutoModelForCausalLM,\n",
    "BitsAndBytesConfig,\n",
    "pipeline\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate, format_document\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableParallel\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "\n",
    "from operator import itemgetter\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "import whisper\n",
    "\n",
    "from whisperspeech.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e390d63-a9e7-46a2-90c6-ded5d73924de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "\n",
    "    #Loading the Mistral Model\n",
    "    model_name='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "\n",
    "    # Building a LLM text-generation pipeline\n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        temperature=0.2,\n",
    "        repetition_penalty=1.1,\n",
    "        return_full_text=True,\n",
    "        max_new_tokens=1024,\n",
    "        device_map = 'auto',\n",
    "    )\n",
    "\n",
    "\n",
    "    return text_generation_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d30bc91-23af-473d-ad4f-78cb1468c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embeddings_model():\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6ab6340-554a-4ed7-9f46-55eca2408f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def text_splitter():\n",
    "    # Simulate some document processing delay\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "027b4b77-9664-4d07-b69f-df944e85b952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_pdfs_to_vectorstore(files):\n",
    "\n",
    "    saved_files_count = 0\n",
    "    documents = []\n",
    "    for file_path in files:\n",
    "        file_name = os.path.basename(file_path)  # Extract the filename from the full path\n",
    "        if file_name.lower().endswith('.pdf'):  # Check if the file is a PDF\n",
    "            saved_files_count += 1\n",
    "            loader_temp = PyPDFLoader(file_path)\n",
    "            docs_temp = loader_temp.load_and_split(text_splitter=textsplitter)\n",
    "            for doc in docs_temp:\n",
    "                # Replace all occurrences of '\\n' with a space ' '\n",
    "                doc.page_content = doc.page_content.replace('\\n', ' ')\n",
    "            documents.append(docs_temp)\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping non-PDF file: {file_name}\")\n",
    "            \n",
    "    global qdrant\n",
    "    \n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents,\n",
    "        HuggingFaceEmbeddings(),\n",
    "        location=\":memory:\", \n",
    "    )\n",
    "\n",
    "    return f\"Added {saved_files_count} PDF file(s) to vectorstore/ You can begin voice chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0efd35b0-2d34-4d5c-abf1-b362f368d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(message):\n",
    "    context_docs = vectorstore.similarity_search(message, k= 3)\n",
    "    context = ' '.join(doc.page_content for doc in context_docs)\n",
    "\n",
    "    template = f\"\"\"Answer the question based only on the following context:\n",
    "        {context}\n",
    "\n",
    "        Question: {message}\n",
    "    \"\"\"\n",
    "\n",
    "    result = llm(template)\n",
    "\n",
    "    answer = result[0][\"generated_text\"].replace(template, '')\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50a4e80-a8ee-4178-82d0-37a4ffa8a7e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0784b0a63e4579b65dcfa5788260bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "t2s-small-en+pl.model:   0%|          | 0.00/856M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b92961f5b91a4ca8857f7e3261e5585c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "s2a-q4-tiny-en+pl.model:   0%|          | 0.00/80.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3538f23cdfd4460cad4cd6dd31e115f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.yaml:   0%|          | 0.00/503 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d65ca8bedc5f40ecaf25c998bbf06684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/40.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/encodec/v0/encodec_24khz-d7cc33bc.th\" to /home/jupyter/.cache/torch/hub/checkpoints/encodec_24khz-d7cc33bc.th\n",
      "100%|██████████| 88.9M/88.9M [00:00<00:00, 96.4MB/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687454d5c917405fa7654e7725629886",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197bb27dc8c643bc8cab46f7d0fff267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4eb28052e79480d945d6f0cc046e9ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda65fae61b841e39affe6eb3acf345b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/72.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a054d582fb894939b813e844cba29cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa860cbb050946c7b1a46f6519e28450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df39a6e23a1457c970c1a9f466d9e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e6d9d6d27d499cac4caab78677eb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca91fa8f9234ede9e94ef35660686b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24dd3819032f4a50bdbfd69b0636d2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "whisper_model = whisper.load_model(\"base\")\n",
    "whisper_speech_model = Pipeline(s2a_ref='collabora/whisperspeech:s2a-q4-tiny-en+pl.model')\n",
    "llm = load_llm()\n",
    "textsplitter = text_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7328d63-39d6-48e0-8f67-e574704ce955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_play_audio(text):\n",
    "    # Construct the directory and filename\n",
    "    directory = '/var/tmp/gradio/'\n",
    "    filename = str(uuid.uuid4()) + \"/audio.wav\"\n",
    "    file_location = os.path.join(directory, filename)\n",
    "    \n",
    "    # Ensure that the directory exists\n",
    "    os.makedirs(os.path.dirname(file_location), exist_ok=True)\n",
    "    \n",
    "    # Generate the audio file from text and save to the specified location\n",
    "    pipe.generate_to_file(file_location, text, lang ='en', cps=15)\n",
    "\n",
    "    # Return the location of the saved audio file for playback\n",
    "    return file_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1606d8f3-ddce-4152-9713-5471c32fc99a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transcribe(audio):\n",
    "    \n",
    "    print(audio)\n",
    "    result = whisper_model.transcribe(audio)\n",
    "    return result[\"text\"]\n",
    "\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5918b954-08d1-49cb-93f4-c56865299565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7869\n",
      "Running on public URL: https://aca0b5e514a9e889a8.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://aca0b5e514a9e889a8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/queueing.py\", line 522, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1741, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1296, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 751, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/tmp/ipykernel_43164/195121366.py\", line 4, in transcribe\n",
      "    result = whisper_model.transcribe(audio)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisper/transcribe.py\", line 122, in transcribe\n",
      "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisper/audio.py\", line 141, in log_mel_spectrogram\n",
      "    audio = torch.from_numpy(audio)\n",
      "TypeError: expected np.ndarray (got NoneType)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/queueing.py\", line 522, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1741, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1296, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 751, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/tmp/ipykernel_43164/195121366.py\", line 4, in transcribe\n",
      "    result = whisper_model.transcribe(audio)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisper/transcribe.py\", line 122, in transcribe\n",
      "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisper/audio.py\", line 141, in log_mel_spectrogram\n",
      "    audio = torch.from_numpy(audio)\n",
      "TypeError: expected np.ndarray (got NoneType)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/tmp/gradio/9b46851155c1b802b80bd56664f594282133cc78/audio.wav\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/queueing.py\", line 522, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/route_utils.py\", line 260, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1741, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/blocks.py\", line 1296, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/to_thread.py\", line 33, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 877, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 807, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/gradio/utils.py\", line 751, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/tmp/ipykernel_43164/195121366.py\", line 4, in transcribe\n",
      "    result = whisper_model.transcribe(audio)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisper/transcribe.py\", line 122, in transcribe\n",
      "    mel = log_mel_spectrogram(audio, model.dims.n_mels, padding=N_SAMPLES)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/whisper/audio.py\", line 141, in log_mel_spectrogram\n",
      "    audio = torch.from_numpy(audio)\n",
      "TypeError: expected np.ndarray (got NoneType)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/var/tmp/gradio/6de593cae8628a2f16662c3413004c8b07a162d7/audio.wav\n"
     ]
    }
   ],
   "source": [
    "with gr.Blocks() as demo:\n",
    "    \n",
    "    with gr.Row():\n",
    "        upload_files = gr.File(label=\"Upload pdf files only\", file_count='multiple')\n",
    "        success_msg = gr.Text(value=\"\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        audio_inp = gr.Audio(sources=\"microphone\", type='filepath')\n",
    "        trans_out = gr.Textbox()\n",
    "    \n",
    "    with gr.Row():\n",
    "        btn_audio = gr.Button(\"Submit Audio\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        model_response = gr.Textbox(label= \"Model Response\")\n",
    "        audio_out = gr.Audio(label=\"AI response in Voice\")\n",
    "        \n",
    "    \n",
    "    upload_files.upload(add_pdfs_to_vectorstore, upload_files, success_msg)\n",
    "    transcribe = btn_audio.click(fn=transcribe, inputs=audio_inp, outputs=trans_out)\n",
    "    answer_gen = transcribe.then(fn=answer_query, inputs= trans_out, outputs= model_response)\n",
    "    answer_gen.then(fn=generate_and_play_audio, inputs= model_response, outputs= audio_out) \n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa5c364-d0b2-414f-93c1-a19111dc56f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-root-py",
   "name": "workbench-notebooks.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
